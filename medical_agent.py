import streamlit as st
import asyncio
import nest_asyncio
import json
import os
import platform
import contextlib
from typing import TypedDict, Annotated, List
from functools import partial

if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# nest_asyncio ì ìš©: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œ ì¤‘ì²© í˜¸ì¶œ í—ˆìš©
nest_asyncio.apply()

# ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì¬ì‚¬ìš©
if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# MCP-Adapter ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage

from langchain_teddynote import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)


logging.langsmith("Medical Search")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¥ ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸", 
    page_icon="ğŸ¥", 
    layout="wide"
)

# config.json íŒŒì¼ ê²½ë¡œ ì„¤ì •
CONFIG_FILE_PATH = "medical_config.json"

# --- ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], lambda x, y: x + y]

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ---
def random_uuid():
    """ëœë¤ UUID ìƒì„±"""
    import uuid
    return str(uuid.uuid4())

def convert_history_to_messages(history, max_turns=8):
    """
    st.session_state.historyë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        history: ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        max_turns: ìµœëŒ€ í¬í•¨í•  ëŒ€í™” í„´ ìˆ˜
    
    Returns:
        List[BaseMessage]: LangChain ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    messages = []
    
    # Medical workflowìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    system_prompt = """ë‹¹ì‹ ì€ ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
- ì˜ë£Œ/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì—°êµ¬ ì •ë³´ ì œê³µ
- ì˜ë£Œ ì •ë³´ì— ëŒ€í•œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ ì œê³µ
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ì—°ì†ì ì¸ ëŒ€í™”
- ë³µì¡í•œ ì˜í•™ ìš©ì–´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
- í•„ìš”ì‹œ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ 

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
- PubMed ê²€ìƒ‰: ì˜í•™/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ ë° ë¶„ì„
- ê¸°íƒ€ ì˜ë£Œ ê´€ë ¨ ë°ì´í„°ë² ì´ìŠ¤

ì¤‘ìš” ì‚¬í•­:
- ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•Šê³ , ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µ
- ì‘ê¸‰ ìƒí™© ì‹œ ì¦‰ì‹œ ì˜ë£Œì§„ì—ê²Œ ì—°ë½í•˜ë„ë¡ ì•ˆë‚´
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ê´€ì„± ìˆëŠ” ë‹µë³€ ì œê³µ
- ë…¼ë¬¸ ê²€ìƒ‰ ì‹œ ì˜ì–´ ë²ˆì—­ì„ í†µí•´ ì •í™•í•œ ê²€ìƒ‰ ìˆ˜í–‰"""
    
    messages.append(SystemMessage(content=system_prompt))
    
    # ìµœê·¼ Ní„´ë§Œ í¬í•¨ (í† í° ì œí•œ ê³ ë ¤)
    # assistant_tool ë©”ì‹œì§€ëŠ” ì œì™¸í•˜ê³  userì™€ assistantë§Œ í¬í•¨
    filtered_history = [msg for msg in history if msg["role"] in ["user", "assistant"]]
    recent_history = filtered_history[-(max_turns*2):] if len(filtered_history) > max_turns*2 else filtered_history
    
    for msg in recent_history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            messages.append(AIMessage(content=msg["content"]))
    
    return messages

def get_conversation_context_summary(history, max_context_turns=3):
    """
    ë²ˆì—­ì„ ìœ„í•œ ëŒ€í™” ë§¥ë½ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        history: ëŒ€í™” ê¸°ë¡
        max_context_turns: í¬í•¨í•  ìµœëŒ€ ë§¥ë½ í„´ ìˆ˜
    
    Returns:
        str: ë§¥ë½ ìš”ì•½ í…ìŠ¤íŠ¸
    """
    if not history:
        return ""
    
    # ìµœê·¼ ëŒ€í™”ë§Œ ì¶”ì¶œ (assistant_tool ì œì™¸)
    filtered_history = [msg for msg in history if msg["role"] in ["user", "assistant"]]
    recent_turns = filtered_history[-(max_context_turns*2):] if len(filtered_history) > max_context_turns*2 else filtered_history
    
    if not recent_turns:
        return ""
    
    context_parts = []
    for msg in recent_turns:
        if msg["role"] == "user":
            context_parts.append(f"ì‚¬ìš©ì: {msg['content'][:100]}...")  # ì²« 100ìë§Œ
        elif msg["role"] == "assistant":
            context_parts.append(f"AI: {msg['content'][:100]}...")  # ì²« 100ìë§Œ
    
    return "\n".join(context_parts)

def load_config_from_json():
    """config.json íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    default_config = {
        "pubmed": {
            "command": "cmd",
            "args": [
                "/c",
                "npx",
                "-y",
                "@smithery/cli@latest",
                "run",
                "@JackKuo666/pubmed-mcp-server",
                "--key",
                "a0fde5b8-88e9-46d3-ab62-ad5096bd7d4b",
            ],
            "transport": "stdio"
        }
    }
    
    try:
        if os.path.exists(CONFIG_FILE_PATH):
            with open(CONFIG_FILE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            save_config_to_json(default_config)
            return default_config
    except Exception as e:
        st.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return default_config

def save_config_to_json(config):
    """ì„¤ì •ì„ config.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(CONFIG_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        st.error(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def get_tool_description_from_available_tools(tool_name: str) -> str | None:
    """ì—°ê²°ëœ ë„êµ¬ë“¤ì—ì„œ ì‹¤ì œ descriptionì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not st.session_state.get('available_tools'):
        return None
    
    # ì—°ê²°ëœ ë„êµ¬ë“¤ì—ì„œ í•´ë‹¹ ì´ë¦„ì˜ ë„êµ¬ ì°¾ê¸°
    for tool in st.session_state.available_tools:
        if tool.name.lower() == tool_name.lower() or tool_name.lower() in tool.name.lower():
            if hasattr(tool, 'description') and tool.description and tool.description.strip():
                return tool.description.strip()
    
    return None  # ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°

# --- LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
async def translate_to_english(state: AgentState, translator_model):
    """í•œê¸€ ì‚¬ìš©ì ì…ë ¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤ (ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤)."""
    messages = state["messages"]
    last_human_message = None
    
    for message in reversed(messages):
        if isinstance(message, HumanMessage):
            last_human_message = message
            break
    
    if last_human_message is None:
        return {"messages": []}
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ ê°€ì ¸ì˜¤ê¸°
    context_summary = get_conversation_context_summary(st.session_state.history)
    
    if context_summary:
        translation_prompt = f"""
        ì´ì „ ëŒ€í™” ë§¥ë½:
        {context_summary}

        í˜„ì¬ ì§ˆë¬¸ì„ ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì˜ë£Œ/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ì— ì í•©í•œ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
        ì˜í•™ ìš©ì–´ëŠ” ì •í™•í•œ ì˜ì–´ ìš©ì–´ë¡œ ë²ˆì—­í•˜ê³ , ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ì í•©í•˜ë„ë¡ ê°„ê²°í•˜ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
        ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ ë²ˆì—­í•˜ë˜, ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

        í˜„ì¬ í•œê¸€ ì§ˆë¬¸: {last_human_message.content}

        ì˜ì–´ ë²ˆì—­:"""
    else:
        translation_prompt = f"""
        ë‹¤ìŒ í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì˜ë£Œ/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ì— ì í•©í•œ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. 
        ì˜í•™ ìš©ì–´ëŠ” ì •í™•í•œ ì˜ì–´ ìš©ì–´ë¡œ ë²ˆì—­í•˜ê³ , ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ì í•©í•˜ë„ë¡ ê°„ê²°í•˜ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

        í•œê¸€ í…ìŠ¤íŠ¸: {last_human_message.content}

        ì˜ì–´ ë²ˆì—­:"""

    response = await translator_model.ainvoke([HumanMessage(content=translation_prompt)])
    translated_text = response.content.strip()
    
    translated_message = HumanMessage(content=translated_text)
    
    new_messages = []
    for message in messages[:-1]:
        new_messages.append(message)
    new_messages.append(translated_message)
    
    return {"messages": new_messages}

async def call_model(state: AgentState, model):
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ë˜ëŠ” ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í•©ë‹ˆë‹¤."""
    messages = state["messages"]
    response = await model.ainvoke(messages)
    return {"messages": [response]}

async def translate_to_korean(state: AgentState, translator_model):
    """ì˜ì–´ AI ì‘ë‹µì„ í•œê¸€ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    messages = state["messages"]
    
    # ë””ë²„ê¹…: ë©”ì‹œì§€ ìƒíƒœ í™•ì¸
    print(f"ğŸ” ë²ˆì—­ ë‹¨ê³„ - ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
    for i, msg in enumerate(messages[-5:]):  # ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ì¶œë ¥
        print(f"  {i}: {type(msg).__name__} - {str(msg)[:100]}...")
    
    last_tool_message = None
    last_ai_message = None
    
    for message in reversed(messages):
        if isinstance(message, ToolMessage) and last_tool_message is None:
            last_tool_message = message
            print(f"âœ… ToolMessage ì°¾ìŒ: {str(message.content)[:100]}...")
        elif isinstance(message, AIMessage) and not message.tool_calls and last_ai_message is None:
            last_ai_message = message
            print(f"âœ… AIMessage ì°¾ìŒ: {str(message.content)[:100]}...")
    
    content_to_translate = None
    translation_source = None
    
    if last_tool_message and last_tool_message.content:
        content_to_translate = last_tool_message.content
        translation_source = "ToolMessage"
    elif last_ai_message and last_ai_message.content:
        content_to_translate = last_ai_message.content
        translation_source = "AIMessage"
    
    print(f"ğŸ¯ ë²ˆì—­í•  ë‚´ìš©: {translation_source} - {str(content_to_translate)[:200] if content_to_translate else 'None'}...")
    
    if content_to_translate is None:
        print("âŒ ë²ˆì—­í•  ë‚´ìš©ì´ ì—†ìŒ - ë¹ˆ ë©”ì‹œì§€ ë°˜í™˜")
        # ë¹ˆ ë©”ì‹œì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        error_message = AIMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. ë²ˆì—­í•  ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        return {"messages": [error_message]}
    
    print("ğŸ”„ ë²ˆì—­ ì‹œì‘...")
    
    translation_prompt = f"""
    ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì˜ë£Œ/ê³¼í•™ ë…¼ë¬¸ ì •ë³´ì˜ ê²½ìš° ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:

    ë²ˆì—­ ê·œì¹™:
    1. PMID, DOI ë“±ì˜ ì‹ë³„ì: ê·¸ëŒ€ë¡œ ìœ ì§€
    2. Title (ì œëª©): ì˜ì–´ ì´ë¦„ ê·¸ëŒ€ë¡œ ìœ ì§€
    3. Authors (ì €ì): ì˜ì–´ ì´ë¦„ ê·¸ëŒ€ë¡œ ìœ ì§€ 
    4. Journal (ì €ë„ëª…): ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€
    5. Publication Date: ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€
    6. Abstract (ì´ˆë¡): í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ë²ˆì—­
    7. ê¸°íƒ€ ë©”íƒ€ë°ì´í„°: ì ì ˆíˆ ë²ˆì—­í•˜ë˜ ì •ë³´ ì†ì‹¤ ì—†ì´ ë³´ì¡´

    ë²ˆì—­í•  í…ìŠ¤íŠ¸:
    {content_to_translate}

    í•œêµ­ì–´ ë²ˆì—­:"""

    try:
        response = await translator_model.ainvoke([HumanMessage(content=translation_prompt)])
        translated_text = response.content.strip()
        
        print(f"âœ… ë²ˆì—­ ì™„ë£Œ: {translated_text[:200]}...")
        
        translated_message = AIMessage(content=translated_text)
        return {"messages": [translated_message]}
        
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        error_message = AIMessage(content=f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"messages": [error_message]}

async def korean_direct_answer(state: AgentState, translator_model):
    """ë„êµ¬ ì‚¬ìš© ì—†ì´ ë°”ë¡œ í•œê¸€ë¡œ ë‹µë³€í•©ë‹ˆë‹¤ - ë©€í‹°í„´ ëŒ€í™” ì§€ì›."""
    messages = state["messages"]
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    context_summary = get_conversation_context_summary(st.session_state.history, max_context_turns=5)
    
    last_human_message = None
    for message in reversed(messages):
        if isinstance(message, HumanMessage):
            last_human_message = message
            break
    
    if last_human_message is None:
        return {"messages": []}
    
    if context_summary:
        korean_prompt = f"""
        ì´ì „ ëŒ€í™” ë§¥ë½:
        {context_summary}

        ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
        ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ê³ , ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¯€ë¡œ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

        í˜„ì¬ ì§ˆë¬¸: {last_human_message.content}

        í•œêµ­ì–´ ë‹µë³€:"""
    else:
        korean_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ì§ì ‘ ë‹µë³€í•´ì£¼ì„¸ìš”. ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë¯€ë¡œ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: {last_human_message.content}

        í•œêµ­ì–´ ë‹µë³€:"""

    response = await translator_model.ainvoke([HumanMessage(content=korean_prompt)])
    korean_answer = response.content.strip()
    
    korean_message = AIMessage(content=korean_answer)
    
    return {"messages": [korean_message]}

# --- ì›Œí¬í”Œë¡œìš° ë¶„ë¥˜ ë° ê´€ë¦¬ ---
class LLMWorkflowClassifier:
    """LLM ê¸°ë°˜ ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš° ë¶„ë¥˜ê¸° - ë„êµ¬ descriptionì„ ì½ê³  reasoningìœ¼ë¡œ íŒë‹¨"""
    
    def __init__(self, llm_model):
        self.llm = llm_model
        self.cache = {}  # ì„±ëŠ¥ ìµœì í™”ìš© ìºì‹œ
    
    def extract_tool_descriptions(self, tools: list) -> list:
        """ë„êµ¬ë“¤ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        tool_info = []
        
        for tool in tools:
            try:
                # ë„êµ¬ì˜ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                info = {
                    "name": tool.name,
                    "description": getattr(tool, 'description', 'ì„¤ëª… ì—†ìŒ'),
                }
                
                # args_schemaì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
                if hasattr(tool, 'args_schema') and tool.args_schema:
                    schema = tool.args_schema
                    if hasattr(schema, '__annotations__'):
                        info["parameters"] = list(schema.__annotations__.keys())
                    elif hasattr(schema, 'model_fields'):
                        info["parameters"] = list(schema.model_fields.keys())
                
                tool_info.append(info)
                
            except Exception as e:
                # ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ í¬í•¨
                tool_info.append({
                    "name": tool.name,
                    "description": "ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨"
                })
        
        return tool_info
    
    def format_tools_for_llm(self, tool_info: list) -> str:
        """ë„êµ¬ ì •ë³´ë¥¼ LLMì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        formatted = []
        
        for i, tool in enumerate(tool_info, 1):
            tool_desc = f"{i}. **{tool['name']}**"
            tool_desc += f"\n   - ì„¤ëª…: {tool['description']}"
            
            if "parameters" in tool and tool["parameters"]:
                tool_desc += f"\n   - ë§¤ê°œë³€ìˆ˜: {', '.join(tool['parameters'])}"
            
            formatted.append(tool_desc)
        
        return "\n\n".join(formatted)
    
    async def classify_workflow(self, query: str, tools: list) -> dict:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° íƒ€ì…ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        
        # 1. ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"{query}_{len(tools)}_{hash(str([t.name for t in tools]))}"
        
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            print(f"ğŸš€ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {cached_result}")
            return cached_result
        
        # 2. ë„êµ¬ ì •ë³´ ì¶”ì¶œ
        tool_info = self.extract_tool_descriptions(tools)
        formatted_tools = self.format_tools_for_llm(tool_info)
        
        # 3. LLM ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        classification_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„ íƒí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        **ì‚¬ìš©ì ì§ˆë¬¸**: "{query}"

        **ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤**:
        {formatted_tools}

        **ì›Œí¬í”Œë¡œìš° ì˜µì…˜**:

1. **medical** (ë§¤ìš° ì œí•œì ìœ¼ë¡œ ì‚¬ìš©):
   - **ì˜¤ì§ ì˜ë£Œ/ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ê³¼ ì—°êµ¬ ë¶„ì„ì—ë§Œ ì‚¬ìš©**
   - PubMed, DOI, PMID ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
   - íŠ¹ì • ì—°êµ¬ ë…¼ë¬¸ì˜ ë‚´ìš© ë¶„ì„ì´ë‚˜ ë©”íƒ€ë¶„ì„
   - í•™ìˆ ì  ì—°êµ¬ ë°ì´í„° ì¡°íšŒ
   - ì˜ì–´ ë²ˆì—­ ê³¼ì •ì´ í¬í•¨ë˜ì–´ ëŠë¦° ì‘ë‹µ
   - ì˜ˆ: "ë‹¹ë‡¨ë³‘ ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰", "PMID 12345 ë…¼ë¬¸ ë‚´ìš©", "ê³ í˜ˆì•• ì¹˜ë£Œ ê´€ë ¨ ì—°êµ¬ ë™í–¥"

2. **general** (ê¸°ë³¸ ì„ íƒ):
   - **ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì´ë¼ë„ ë…¼ë¬¸ ê²€ìƒ‰ì´ ì•„ë‹ˆë©´ ì—¬ê¸° ì‚¬ìš©**
   - ì¼ë°˜ì ì¸ ì˜ë£Œ ì •ë³´, ì¦ìƒ, ì¹˜ë£Œë²•, ê±´ê°• ìƒì‹
   - ì‹¤ì‹œê°„ ì˜ë£Œ ë‰´ìŠ¤, ë³‘ì› ì •ë³´, ì•½ë¬¼ ì •ë³´
   - ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì˜ë£Œ ì •ë³´ ì¡°íšŒ
   - ë¹ ë¥¸ ì‘ë‹µê³¼ ë‹¤ì–‘í•œ ë„êµ¬ í™œìš© ê°€ëŠ¥
   - ë‰´ìŠ¤, ë‚ ì”¨, ì‹œê°„, ê³„ì‚°, ì¼ë°˜ ê²€ìƒ‰ ë“±
   - ì˜ˆ: "ê°ê¸° ì¦ìƒê³¼ ì¹˜ë£Œë²•", "ê·¼ì²˜ ë³‘ì› ì°¾ê¸°", "í˜ˆì••ì•½ ë¶€ì‘ìš©", "ê±´ê°•í•œ ì‹ë‹¨", "ì˜ë£Œ ë‰´ìŠ¤"

        **ì¤‘ìš”í•œ ë¶„ë¥˜ ê¸°ì¤€**:
        1. **ë…¼ë¬¸/ì—°êµ¬ ê²€ìƒ‰ ì—¬ë¶€**: í•™ìˆ  ë…¼ë¬¸ì´ë‚˜ ì—°êµ¬ ë°ì´í„°ë¥¼ ì°¾ëŠ” ì§ˆë¬¸ì¸ê°€?
        2. **PMID/DOI ì–¸ê¸‰**: íŠ¹ì • ë…¼ë¬¸ ì‹ë³„ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
        3. **ì—°êµ¬ ë™í–¥/ë©”íƒ€ë¶„ì„**: íŠ¹ì • ì£¼ì œì˜ ì—°êµ¬ ë™í–¥ì´ë‚˜ ë¶„ì„ì„ ìš”êµ¬í•˜ëŠ”ê°€?
        
        **ì£¼ì˜ì‚¬í•­**:
        - ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì´ë¼ë„ ë…¼ë¬¸ ê²€ìƒ‰ì´ ì•„ë‹ˆë©´ **ë°˜ë“œì‹œ general ì„ íƒ**
        - ì¼ë°˜ì ì¸ ì˜ë£Œ ì •ë³´ë‚˜ ê±´ê°• ìƒì‹ì€ general ì›Œí¬í”Œë¡œìš°ê°€ ë” ì í•©
        - í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ generalì„ ì„ íƒ (ë” ë¹ ë¥´ê³  ìœ ì—°í•¨)

        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
        {{
            "workflow": "medical" ë˜ëŠ” "general",
            "confidence": 0.0-1.0 ì‚¬ì´ì˜ í™•ì‹ ë„,
            "reason": "ì„ íƒí•œ ì´ìœ ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
        }}"""

        try:
            # 4. LLM í˜¸ì¶œ
            print(f"ğŸ§  LLM ì›Œí¬í”Œë¡œìš° ë¶„ë¥˜ ì‹œì‘...")
            
            response = await asyncio.wait_for(
                self.llm.ainvoke([HumanMessage(content=classification_prompt)]),
                timeout=15.0  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
            )
            
            # 5. ì‘ë‹µ íŒŒì‹±
            result = self.parse_llm_response(response.content)
            
            # 6. ìºì‹±
            self.cache[cache_key] = result
            
            print(f"ğŸ¯ LLM ë¶„ë¥˜ ê²°ê³¼: {result}")
            return result
            
        except asyncio.TimeoutError:
            print("â° LLM ë¶„ë¥˜ íƒ€ì„ì•„ì›ƒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self.get_fallback_classification(query, tool_info)
        except Exception as e:
            print(f"âŒ LLM ë¶„ë¥˜ ì˜¤ë¥˜: {str(e)} - ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self.get_fallback_classification(query, tool_info)
    
    def parse_llm_response(self, response_content: str) -> dict:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ê²°ê³¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if "workflow" in result and result["workflow"] in ["medical", "general"]:
                    return {
                        "workflow": result["workflow"],
                        "confidence": result.get("confidence", 0.8),
                        "reason": result.get("reason", "LLM ë¶„ë¥˜"),
                        "method": "llm"
                    }
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
            if "medical" in response_content.lower():
                return {"workflow": "medical", "confidence": 0.7, "reason": "í…ìŠ¤íŠ¸ íŒŒì‹±", "method": "fallback"}
            else:
                return {"workflow": "general", "confidence": 0.7, "reason": "í…ìŠ¤íŠ¸ íŒŒì‹±", "method": "fallback"}
                
        except Exception as e:
            print(f"âš ï¸ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return {"workflow": "general", "confidence": 0.5, "reason": "íŒŒì‹± ì‹¤íŒ¨", "method": "error"}
    
    def get_fallback_classification(self, query: str, tool_info: list) -> dict:
        """LLM ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê°„ë‹¨í•œ fallback ë¡œì§"""
        # LLM ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ general ì›Œí¬í”Œë¡œìš°ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        return {"workflow": "general", "confidence": 0.5, "reason": "LLM ë¶„ë¥˜ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©", "method": "fallback"}

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€
class WorkflowClassifier:
    """ê¸°ì¡´ WorkflowClassifierì˜ í˜¸í™˜ì„± ë˜í¼ - ì´ì œ LLM reasoning ì‚¬ìš©"""
    
    _llm_classifier = None
    
    @staticmethod
    async def initialize_llm_classifier():
        """LLM ë¶„ë¥˜ê¸° ì´ˆê¸°í™”"""
        if WorkflowClassifier._llm_classifier is None:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            WorkflowClassifier._llm_classifier = LLMWorkflowClassifier(llm)
    
    @staticmethod
    async def select_workflow_type(query: str, tools: list) -> str:
        """ğŸ§  LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì„ íƒ - ë„êµ¬ descriptionì„ ì½ê³  reasoningìœ¼ë¡œ íŒë‹¨"""
        await WorkflowClassifier.initialize_llm_classifier()
        
        try:
            result = await WorkflowClassifier._llm_classifier.classify_workflow(query, tools)
            print(f"ğŸ¯ ìµœì¢…ì„ íƒ: {result['workflow']} (í™•ì‹ ë„: {result['confidence']:.2f}, ë°©ë²•: {result['method']}, ì´ìœ : {result['reason']})")
            return result["workflow"]
        except Exception as e:
            print(f"âŒ LLM ë¶„ë¥˜ ì‹¤íŒ¨, ê·¹ë‹¨ì  fallback ì‚¬ìš©: {str(e)}")
            # ê·¹ë‹¨ì  fallback - ì•ˆì „í•˜ê²Œ general ê¸°ë³¸ê°’ ì‚¬ìš©
            return "general"

class WorkflowFactory:
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def create_medical_workflow(tools: list, model, translator_model, mcp_config):
        """ì˜ë£Œ ì „ìš© ì›Œí¬í”Œë¡œìš° ìƒì„± (ê¸°ì¡´ ë¡œì§)"""
        workflow = StateGraph(AgentState)
        
        workflow.add_node("translate_to_english", partial(translate_to_english, translator_model=translator_model))
        workflow.add_node("agent", partial(call_model, model=model.bind_tools(tools)))
        
        # ì•ˆì „í•œ ë„êµ¬ ë…¸ë“œ
        async def safe_tool_node(state):
            try:
                async with connect_all_mcp_servers(mcp_config) as (fresh_tools, _):
                    if fresh_tools:
                        tool_node = ToolNode(fresh_tools)
                        return await tool_node.ainvoke(state)
            except Exception as e:
                error_msg = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                return {"messages": [ToolMessage(content=error_msg, tool_call_id="error")]}
        
        workflow.add_node("action", safe_tool_node)
        workflow.add_node("translate_to_korean", partial(translate_to_korean, translator_model=translator_model))
        workflow.add_node("direct_answer", partial(korean_direct_answer, translator_model=translator_model))

        # ì›Œí¬í”Œë¡œìš° ì—°ê²°
        workflow.set_entry_point("translate_to_english")
        workflow.add_edge("translate_to_english", "agent")
        workflow.add_conditional_edges(
            "agent", 
            should_continue_or_answer_medical, 
            {"continue": "action", "direct_answer": "direct_answer"}
        )
        workflow.add_conditional_edges(
            "action", after_action_medical,
            {"agent": "agent", "translate": "translate_to_korean"}
        )
        workflow.add_edge("translate_to_korean", END)
        workflow.add_edge("direct_answer", END)
        
        return workflow
    
    @staticmethod  
    def create_general_workflow(tools: list, model, translator_model, mcp_config):
        """ì¼ë°˜ ë„êµ¬ìš© ì›Œí¬í”Œë¡œìš° ìƒì„± (ë²ˆì—­ ì—†ìŒ)"""
        workflow = StateGraph(AgentState)
        
        # í•œê¸€ ì§ì ‘ ì²˜ë¦¬ ì—ì´ì „íŠ¸ (ë©€í‹°í„´ ì§€ì›)
        async def korean_agent(state):
            """í•œê¸€ë¡œ ì§ì ‘ ì²˜ë¦¬í•˜ëŠ” ì—ì´ì „íŠ¸ - ë©€í‹°í„´ ëŒ€í™” ì§€ì›"""
            messages = state["messages"]
            
            # General workflowìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ë‹¤ë©´ ì¶”ê°€
            has_system_prompt = any(isinstance(msg, SystemMessage) for msg in messages)
            
            if not has_system_prompt:
                general_system_prompt = """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

                ì£¼ìš” ì—­í• :
                - ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ ë° ì œê³µ (ë‰´ìŠ¤, ë‚ ì”¨, ì‹œê°„ ë“±)
                - ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€
                - ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ì—°ì†ì ì¸ ëŒ€í™”
                - ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ í™œìš©

                ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
                - ì›¹ ê²€ìƒ‰: ìµœì‹  ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì»¤ë®¤ë‹ˆí‹° ë“± ë‹¤ì–‘í•œ ì •ë³´ ê²€ìƒ‰
                - ë‚ ì”¨ ì •ë³´: í˜„ì¬ ë‚ ì”¨ ë° ì˜ˆë³´ ì •ë³´
                - ì‹œê°„ ì¡°íšŒ: í˜„ì¬ ì‹œê°„ ë° ì‹œê°„ëŒ€ ì •ë³´
                - ê¸°íƒ€ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ

                ì¤‘ìš” ì‚¬í•­:
                - ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ê´€ì„± ìˆëŠ” ë‹µë³€ ì œê³µ
                - ì‹¤ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì ì ˆí•œ ë„êµ¬ ì‚¬ìš©
                - ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ ì œê³µì— ì¤‘ì 
                - ì—°ê²°ëœ ë„êµ¬ë“¤ì˜ ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©"""
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§¨ ì•ì— ì¶”ê°€
                messages = [SystemMessage(content=general_system_prompt)] + messages
                state = {"messages": messages}
            
            return await call_model(state, model.bind_tools(tools))
        
        # ê°„ë‹¨í•œ ë„êµ¬ ë…¸ë“œ
        async def simple_tool_node(state):
            try:
                async with connect_all_mcp_servers(mcp_config) as (fresh_tools, _):
                    if fresh_tools:
                        tool_node = ToolNode(fresh_tools)
                        return await tool_node.ainvoke(state)
            except Exception as e:
                error_msg = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                return {"messages": [ToolMessage(content=error_msg, tool_call_id="error")]}
        
        # í•œê¸€ ì‘ë‹µ ì •ë¦¬
        async def format_korean_response(state):
            """í•œê¸€ ì‘ë‹µì„ ì •ë¦¬í•˜ëŠ” ë…¸ë“œ"""
            messages = state["messages"]
            
            # ë§ˆì§€ë§‰ ToolMessageë‚˜ AIMessage ì°¾ê¸°
            last_content = None
            for message in reversed(messages):
                if isinstance(message, ToolMessage) and message.content:
                    last_content = message.content
                    break
                elif isinstance(message, AIMessage) and message.content and not message.tool_calls:
                    last_content = message.content
                    break
            
            if last_content:
                # ê°„ë‹¨í•œ í¬ë§·íŒ… (í•„ìš”ì‹œ)
                formatted_content = last_content
                return {"messages": [AIMessage(content=formatted_content)]}
            else:
                return {"messages": [AIMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}
        
        workflow.add_node("agent", korean_agent)
        workflow.add_node("action", simple_tool_node)
        workflow.add_node("format_response", format_korean_response)
        workflow.add_node("direct_answer", partial(korean_direct_answer, translator_model=translator_model))

        # ì›Œí¬í”Œë¡œìš° ì—°ê²° (ë‹¨ìˆœí™”)
        workflow.set_entry_point("agent")
        workflow.add_conditional_edges(
            "agent",
            should_continue_or_answer_general,
            {"continue": "action", "direct_answer": "direct_answer"}
        )
        workflow.add_edge("action", "format_response")
        workflow.add_edge("format_response", END)
        workflow.add_edge("direct_answer", END)
        
        return workflow

# --- ì¡°ê±´ë¶€ ì—£ì§€ ë¡œì§ ---
def should_continue_or_answer_medical(state: AgentState) -> str:
    """ì˜ë£Œ ì›Œí¬í”Œë¡œìš°ìš© - LLMì˜ ì‘ë‹µì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "continue"
    return "direct_answer"

def should_continue_or_answer_general(state: AgentState) -> str:
    """ì¼ë°˜ ì›Œí¬í”Œë¡œìš°ìš© - LLMì˜ ì‘ë‹µì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "continue"
    return "direct_answer"

def after_action_medical(state: AgentState) -> str:
    """ì˜ë£Œ ì›Œí¬í”Œë¡œìš°ìš© - Action í›„ì—ëŠ” í•­ìƒ ë²ˆì—­ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    return "translate"

# General workflowëŠ” after_actionì´ í•„ìš” ì—†ìŒ (ë°”ë¡œ format_responseë¡œ ì´ë™)

# --- MCP ì„œë²„ ê´€ë¦¬ ---
def update_server_status(server_name, status, error_message="", tools_count=0):
    """ì„œë²„ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    import time
    st.session_state.server_status[server_name] = {
        "status": status,
        "tools_count": tools_count,
        "error_message": error_message,
        "last_updated": time.time()
    }
    
    # ë¡œê·¸ ì¶”ê°€
    log_message = f"[{server_name}] {status}"
    if error_message:
        log_message += f": {error_message}"
    st.session_state.connection_logs.append(log_message)

# ê¸€ë¡œë²Œ ì—°ê²° ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
_global_mcp_connections = {}

class MCPConnectionManager:
    """MCP ì—°ê²°ì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    async def create_connection(server_name, server_config):
        """ìƒˆë¡œìš´ MCP ì„œë²„ ì—°ê²°ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        update_server_status(server_name, "connecting", "ì„œë²„ ì—°ê²° ì‹œë„ ì¤‘...")
        
        try:
            params = StdioServerParameters(
                command=server_config['command'],
                args=server_config['args']
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì—°ê²°
            async def _connection_generator():
                async with stdio_client(params) as (read, write):
                    async with ClientSession(read, write) as session:
                        try:
                            await asyncio.wait_for(session.initialize(), timeout=30.0)
                            update_server_status(server_name, "connecting", "ë„êµ¬ ë¡œë“œ ì¤‘...")
                            
                            tools = await load_mcp_tools(session)
                            tool_count = len(tools)
                            update_server_status(server_name, "connected", f"{tool_count}ê°œ ë„êµ¬ ì—°ê²° ì„±ê³µ", tool_count)
                            
                            # ë¬´í•œ ëŒ€ê¸°í•˜ì—¬ ì—°ê²° ìœ ì§€
                            yield tools, session
                            
                        except asyncio.TimeoutError:
                            error_msg = "ì„œë²„ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)"
                            update_server_status(server_name, "failed", error_msg)
                            raise Exception(error_msg)
                        except Exception as e:
                            error_msg = f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                            update_server_status(server_name, "failed", error_msg)
                            raise Exception(error_msg)
            
            return _connection_generator()
            
        except Exception as e:
            error_msg = f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}"
            update_server_status(server_name, "failed", error_msg)
            raise Exception(error_msg)

async def connect_all_mcp_servers_simple(mcp_config):
    """ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ MCP ì„œë²„ì— ì—°ê²°í•©ë‹ˆë‹¤."""
    all_tools = []
    
    # ì—°ê²° ë¡œê·¸ ì´ˆê¸°í™”
    st.session_state.connection_logs = []
    
    try:
        # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©í•˜ë˜ ë” ê°„ë‹¨í•˜ê²Œ
        async with connect_all_mcp_servers(mcp_config) as (tools, server_sessions):
            all_tools = tools
            # ë„êµ¬ë§Œ ë°˜í™˜í•˜ê³  ì—°ê²° ê´€ë¦¬ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return all_tools
            
    except Exception as e:
        st.error(f"âŒ MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return []

# ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € í•¨ìˆ˜ëŠ” ìœ ì§€í•˜ë˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
@contextlib.asynccontextmanager
async def connect_mcp_server(server_name, server_config):
    """í•˜ë‚˜ì˜ MCP ì„œë²„ì— ì—°ê²°í•˜ê³  ë„êµ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    update_server_status(server_name, "connecting", "ì„œë²„ ì—°ê²° ì‹œë„ ì¤‘...")
    
    try:
        params = StdioServerParameters(
            command=server_config['command'],
            args=server_config['args']
        )
        
        async with stdio_client(params) as (read, write):
            update_server_status(server_name, "connecting", "í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
            
            async with ClientSession(read, write) as session:
                try:
                    await asyncio.wait_for(session.initialize(), timeout=30.0)
                    update_server_status(server_name, "connecting", "ë„êµ¬ ë¡œë“œ ì¤‘...")
                    
                    tools = await load_mcp_tools(session)
                    tool_count = len(tools)
                    update_server_status(server_name, "connected", f"{tool_count}ê°œ ë„êµ¬ ì—°ê²° ì„±ê³µ", tool_count)
                    
                    yield tools, session
                    
                except asyncio.TimeoutError:
                    error_msg = "ì„œë²„ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)"
                    update_server_status(server_name, "failed", error_msg)
                    raise Exception(error_msg)
                except Exception as e:
                    error_msg = f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                    update_server_status(server_name, "failed", error_msg)
                    raise Exception(error_msg)
    
    except Exception as e:
        error_msg = f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}"
        update_server_status(server_name, "failed", error_msg)
        raise Exception(error_msg)

@contextlib.asynccontextmanager
async def connect_all_mcp_servers(mcp_config):
    """ëª¨ë“  MCP ì„œë²„ì— ë™ì‹œì— ì—°ê²°í•˜ê³  ëª¨ë“  ë„êµ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    all_tools = []
    server_sessions = {}
    
    # ì—°ê²° ë¡œê·¸ ì´ˆê¸°í™”
    st.session_state.connection_logs = []
    
    async with contextlib.AsyncExitStack() as stack:
        for server_id, config in mcp_config.items():
            try:
                tools, session = await stack.enter_async_context(connect_mcp_server(server_id, config))
                all_tools.extend(tools)
                server_sessions[server_id] = {
                    'session': session,
                    'tools': tools,
                    'name': server_id
                }
            except Exception as e:
                # ìƒíƒœëŠ” ì´ë¯¸ connect_mcp_serverì—ì„œ ì—…ë°ì´íŠ¸ë¨
                continue
        
        yield all_tools, server_sessions

# --- ìŠ¤íŠ¸ë¦¬ë° ê´€ë ¨ í•¨ìˆ˜ë“¤ ---
def get_streaming_callback(text_placeholder, tool_placeholder):
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    accumulated_text = []
    accumulated_tool = []

    def callback_func(message: dict):
        nonlocal accumulated_text, accumulated_tool
        message_content = message.get("content", None)

        # AIMessage ì²˜ë¦¬ (ë²ˆì—­ ê²°ê³¼ ë“±)
        if isinstance(message_content, AIMessage):
            if message_content.content and not message_content.tool_calls:
                # ë²ˆì—­ëœ ê²°ê³¼ë‚˜ ì§ì ‘ ë‹µë³€
                accumulated_text.append(message_content.content)
                text_placeholder.markdown("".join(accumulated_text))
                print(f"ğŸ“ AIMessage ì²˜ë¦¬: {message_content.content[:100]}...")
        
        # AIMessageChunk ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë°)
        elif isinstance(message_content, AIMessageChunk):
            content = message_content.content
            if isinstance(content, list) and len(content) > 0:
                message_chunk = content[0]
                if message_chunk["type"] == "text":
                    accumulated_text.append(message_chunk["text"])
                    text_placeholder.markdown("".join(accumulated_text))
                elif message_chunk["type"] == "tool_use":
                    if "partial_json" in message_chunk:
                        accumulated_tool.append(message_chunk["partial_json"])
                    else:
                        tool_call_chunks = message_content.tool_call_chunks
                        tool_call_chunk = tool_call_chunks[0]
                        accumulated_tool.append(
                            "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                        )
                    with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                        st.markdown("".join(accumulated_tool))
            elif isinstance(content, str):
                accumulated_text.append(content)
                text_placeholder.markdown("".join(accumulated_text))
        
        # ToolMessage ì²˜ë¦¬
        elif isinstance(message_content, ToolMessage):
            accumulated_tool.append(
                "\n```json\n" + str(message_content.content) + "\n```\n"
            )
            with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                st.markdown("".join(accumulated_tool))
        
        return None

    return callback_func, accumulated_text, accumulated_tool

async def astream_graph(graph, inputs, callback=None, config=None):
    """ê·¸ë˜í”„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    final_state = None
    async for output in graph.astream(inputs, config=config, stream_mode="values"):
        final_state = output
        if callback:
            callback({"content": final_state["messages"][-1]})
    
    return final_state

# --- ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---
async def cleanup_mcp_client():
    """ê¸°ì¡´ MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    # ê°„ë‹¨í•œ ìƒíƒœ ì •ë¦¬ë§Œ ìˆ˜í–‰
    if "mcp_connections" in st.session_state:
        st.session_state.mcp_connections = None
    
    # ê¸°íƒ€ ê´€ë ¨ ìƒíƒœë“¤ë„ ì •ë¦¬
    st.session_state.session_initialized = False
    st.session_state.agent = None

def print_message():
    """ì±„íŒ… ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤."""
    i = 0
    while i < len(st.session_state.history):
        message = st.session_state.history[i]

        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(message["content"])
            i += 1
        elif message["role"] == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¥"):
                st.markdown(message["content"])
                if (
                    i + 1 < len(st.session_state.history)
                    and st.session_state.history[i + 1]["role"] == "assistant_tool"
                ):
                    with st.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                        st.markdown(st.session_state.history[i + 1]["content"])
                    i += 2
                else:
                    i += 1
        else:
            i += 1

async def process_basic_query(query, text_placeholder, timeout_seconds=120):
    """MCP ë„êµ¬ ì—†ì´ ê¸°ë³¸ ëª¨ë¸ë§Œìœ¼ë¡œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë©€í‹°í„´ ì§€ì›)."""
    try:
        if st.session_state.basic_model:
            with text_placeholder.container():
                # ì´ì „ ëŒ€í™”ê°€ ìˆëŠ”ì§€ í™•ì¸
                if st.session_state.history:
                    st.write("ğŸ§  ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ë©° ë‹µë³€ ìƒì„± ì¤‘...")
                else:
                    st.write("ğŸ¤– ê¸°ë³¸ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ë©”ì‹œì§€ ì²´ì¸ êµ¬ì„±
            max_turns = st.session_state.get('conversation_memory_turns', 8)
            messages = convert_history_to_messages(st.session_state.history, max_turns)
            
            # í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
            messages.append(HumanMessage(content=query))
            
            response = await asyncio.wait_for(
                st.session_state.basic_model.ainvoke(messages),
                timeout=timeout_seconds,
            )
            
            final_text = response.content
            return {"success": True}, final_text, ""
        else:
            return (
                {"error": "ğŸš« ê¸°ë³¸ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ê¸°ë³¸ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
    except asyncio.TimeoutError:
        error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
        return {"error": error_msg}, error_msg, ""
    except Exception as e:
        import traceback
        error_msg = f"âŒ ê¸°ë³¸ ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        return {"error": error_msg}, error_msg, ""

async def process_query_smart(query, text_placeholder, tool_placeholder, timeout_seconds=120):
    """ìŠ¤ë§ˆíŠ¸ ì›Œí¬í”Œë¡œìš° ì„ íƒìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        if not st.session_state.session_initialized:
            return (
                {"error": "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
        
        # 1. ì›Œí¬í”Œë¡œìš° íƒ€ì… ë™ì  ì„ íƒ - LLM reasoning ì‚¬ìš©
        tools = st.session_state.get('available_tools', [])
        selected_workflow = await WorkflowClassifier.select_workflow_type(query, tools)
        
        # 2. ì„ íƒëœ ì›Œí¬í”Œë¡œìš°ì— ë”°ë¼ ì—ì´ì „íŠ¸ ìƒì„±
        if selected_workflow == "medical":
            agent = WorkflowFactory.create_medical_workflow(
                tools, 
                st.session_state.main_model, 
                st.session_state.translator_model,
                st.session_state.mcp_config
            ).compile(checkpointer=MemorySaver())
            progress_message = "ğŸ”„ ì˜ë£Œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ (í•œê¸€ â†’ ì˜ì–´ ë²ˆì—­ â†’ ê²€ìƒ‰ â†’ í•œê¸€ ë²ˆì—­)..."
        else:
            agent = WorkflowFactory.create_general_workflow(
                tools,
                st.session_state.main_model,
                st.session_state.translator_model, 
                st.session_state.mcp_config
            ).compile(checkpointer=MemorySaver())
            progress_message = "ğŸ”„ ì¼ë°˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ (ì§ì ‘ ì²˜ë¦¬)..."
        
        # 3. ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì„¤ì •
        streaming_callback, accumulated_text_obj, accumulated_tool_obj = (
            get_streaming_callback(text_placeholder, tool_placeholder)
        )
        
        try:
            # 4. ì§„í–‰ ìƒí™© í‘œì‹œ
            with text_placeholder.container():
                st.write(progress_message)
            
            # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•´ ì´ì „ ëŒ€í™” ê¸°ë¡ í¬í•¨
            max_turns = st.session_state.get('conversation_memory_turns', 8)
            messages = convert_history_to_messages(st.session_state.history, max_turns)
            
            # í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
            messages.append(HumanMessage(content=query))
            
            inputs = {"messages": messages}
            
            # 5. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            response = await asyncio.wait_for(
                astream_graph(
                    agent,
                    inputs,
                    callback=streaming_callback,
                    config=RunnableConfig(
                        recursion_limit=st.session_state.recursion_limit,
                        thread_id=st.session_state.thread_id,
                    ),
                ),
                timeout=timeout_seconds,
            )
        except asyncio.TimeoutError:
            error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            return {"error": error_msg}, error_msg, ""

        # 6. ê²°ê³¼ ì²˜ë¦¬
        streaming_text = "".join(accumulated_text_obj)
        final_tool = "".join(accumulated_tool_obj)
        
        # ìµœì¢… ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
        final_text = streaming_text
        if response and "messages" in response:
            # ë§ˆì§€ë§‰ AIMessage ì°¾ê¸°
            last_ai_message = None
            for message in reversed(response["messages"]):
                if isinstance(message, AIMessage) and message.content and not message.tool_calls:
                    last_ai_message = message
                    break
            
            if last_ai_message:
                final_text = last_ai_message.content
                print(f"âœ… ìµœì¢… í…ìŠ¤íŠ¸ ì¶”ì¶œ ({selected_workflow}): {final_text[:200]}...")
                
                # UIì—ë„ í‘œì‹œ
                with text_placeholder.container():
                    st.markdown(final_text)
            elif streaming_text:
                final_text = streaming_text
                print(f"ğŸ“ ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì‚¬ìš© ({selected_workflow}): {final_text[:200]}...")
            else:
                print("âš ï¸ ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                final_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        return response, final_text, final_tool
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""

# ê¸°ì¡´ í•¨ìˆ˜ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=120):
    """ê¸°ì¡´ process_query í•¨ìˆ˜ - ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸ ë²„ì „ í˜¸ì¶œ"""
    return await process_query_smart(query, text_placeholder, tool_placeholder, timeout_seconds)

async def initialize_basic_model():
    """MCP ë„êµ¬ ì—†ì´ ê¸°ë³¸ ëª¨ë¸ë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        selected_model = st.session_state.selected_model

        model = ChatOpenAI(
            model=selected_model,
            temperature=0.1,
            max_tokens=4096,
        )

        st.session_state.basic_model = model
        st.session_state.basic_model_initialized = True
        return True
    except Exception as e:
        st.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

class ToolManager:
    """ë„êµ¬ë¥¼ ê´€ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ í´ë˜ìŠ¤"""
    
    @staticmethod
    async def get_tools_with_retry(mcp_config, max_retries=2):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë„êµ¬ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                async with connect_all_mcp_servers(mcp_config) as (tools, server_sessions):
                    return tools
            except Exception as e:
                if attempt == max_retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                    raise e
                await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        return []

async def initialize_session(mcp_config=None):
    """MCP ì„¸ì…˜ê³¼ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with st.spinner("ğŸ”„ AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘..."):
        # ê¸°ì¡´ ìƒíƒœ ì •ë¦¬
        st.session_state.session_initialized = False
        st.session_state.agent = None

        if mcp_config is None:
            mcp_config = load_config_from_json()

        try:
            # ì—°ê²°í•  ì„œë²„ë“¤ì„ ë¯¸ë¦¬ ì—°ê²° ì‹œë„ ìƒíƒœë¡œ ì„¤ì •
            print("ğŸ”„ MCP ì„œë²„ ì—°ê²° ì‹œì‘...")
            for server_name in mcp_config.keys():
                update_server_status(server_name, "connecting", "ì„œë²„ ì—°ê²° ì‹œë„ ì¤‘...")
            
            # ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ë„êµ¬ë§Œ ê°€ì ¸ì˜¤ê¸°
            all_tools = await ToolManager.get_tools_with_retry(mcp_config)
            
            if not all_tools:
                # ëª¨ë“  ì„œë²„ ì—°ê²° ì‹¤íŒ¨ë¡œ í‘œì‹œ
                for server_name in mcp_config.keys():
                    update_server_status(server_name, "failed", "ë„êµ¬ ë¡œë“œ ì‹¤íŒ¨ - ì—°ê²°ëœ ë„êµ¬ ì—†ìŒ")
                st.warning("âš ï¸ ì—°ê²°ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
                return False
            
            st.session_state.tool_count = len(all_tools)
            st.session_state.available_tools = all_tools
            st.session_state.mcp_config = mcp_config

            # OpenAI ëª¨ë¸ ì´ˆê¸°í™”
            selected_model = st.session_state.selected_model

            model = ChatOpenAI(
                model=selected_model,
                temperature=0.1,
                max_tokens=4096,
            )
            translator_model = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0,
                max_tokens=4096,
            )

            # ëª¨ë¸ë“¤ì„ ì„¸ì…˜ì— ì €ì¥
            st.session_state.main_model = model
            st.session_state.translator_model = translator_model

            # LLM ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
            await WorkflowClassifier.initialize_llm_classifier()
            
            # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° íƒ€ì…ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„¤ì • (ë™ì  ì„ íƒ ì‚¬ìš©)
            st.session_state.default_workflow_type = "general"
            
            print(f"ğŸ§  LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"ğŸ¯ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° íƒ€ì…: general (ì§ˆë¬¸ë³„ë¡œ ë™ì  ì„ íƒ)")
            
            # ê¸°ë³¸ ì—ì´ì „íŠ¸ëŠ” generalë¡œ ì„¤ì • (ì‹¤ì œë¡œëŠ” ë™ì  ì„ íƒ)
            agent = WorkflowFactory.create_general_workflow(
                all_tools, model, translator_model, mcp_config
            ).compile(checkpointer=MemorySaver())

            st.session_state.agent = agent
            st.session_state.session_initialized = True
            
            print(f"âœ… ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - ì´ {len(all_tools)}ê°œ ë„êµ¬ ì—°ê²°ë¨")
            return True
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            
            # ì—°ê²° ì‹¤íŒ¨í•œ ì„œë²„ë“¤ì„ ì‹¤íŒ¨ ìƒíƒœë¡œ í‘œì‹œ
            for server_name in mcp_config.keys():
                current_status = st.session_state.server_status.get(server_name, {}).get("status", "unknown")
                if current_status != "connected":  # ì´ë¯¸ ì—°ê²°ëœ ì„œë²„ê°€ ì•„ë‹ˆë¼ë©´ ì‹¤íŒ¨ë¡œ í‘œì‹œ
                    update_server_status(server_name, "failed", f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            st.error(f"âŒ MCP ì„œë²„ ì—°ê²° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            st.error(f"ìƒì„¸ ì˜¤ë¥˜: {error_details}")
            return False

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "session_initialized" not in st.session_state:
    st.session_state.session_initialized = False
    st.session_state.agent = None
    st.session_state.history = []
    st.session_state.mcp_connections = None  # ì‹¤ì œ MCP ì—°ê²° ê°ì²´ë“¤ ì €ì¥
    st.session_state.timeout_seconds = 120
    st.session_state.selected_model = "gpt-4o"
    st.session_state.recursion_limit = 100
    st.session_state.tool_count = 0
    st.session_state.basic_model = None
    st.session_state.basic_model_initialized = False
    st.session_state.server_status = {}  # ì„œë²„ë³„ ì—°ê²° ìƒíƒœ ê´€ë¦¬
    st.session_state.connection_logs = []  # ì—°ê²° ë¡œê·¸
    st.session_state.conversation_memory_turns = 8  # ê¸°ì–µí•  ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜
    
    # ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ ë³€ìˆ˜ë“¤
    st.session_state.available_tools = []  # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
    st.session_state.main_model = None  # ë©”ì¸ ëª¨ë¸
    st.session_state.translator_model = None  # ë²ˆì—­ ëª¨ë¸
    st.session_state.mcp_config = None  # MCP ì„¤ì •
    st.session_state.default_workflow_type = "general"  # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° íƒ€ì…

if "thread_id" not in st.session_state:
    st.session_state.thread_id = random_uuid()

if "pending_mcp_config" not in st.session_state:
    st.session_state.pending_mcp_config = load_config_from_json()

# ê¸°ë³¸ ëª¨ë¸ ìë™ ì´ˆê¸°í™”
if not st.session_state.basic_model_initialized:
    st.session_state.event_loop.run_until_complete(initialize_basic_model())

# --- ë©”ì¸ UI ---
st.title("ğŸ¤– AI ì—ì´ì „íŠ¸")
st.markdown("âœ¨ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# --- ì‚¬ì´ë“œë°” êµ¬ì„± ---
with st.sidebar:
    st.subheader("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ (OpenAIë§Œ)
    has_openai_key = os.environ.get("OPENAI_API_KEY") is not None
    
    if has_openai_key:
        available_models = ["gpt-4o", "gpt-4o-mini"]
    else:
        st.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— API í‚¤ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        available_models = ["gpt-4o"]

    previous_model = st.session_state.selected_model
    st.session_state.selected_model = st.selectbox(
        "ğŸ¤– ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì„ íƒ",
        options=available_models,
        index=(
            available_models.index(st.session_state.selected_model)
            if st.session_state.selected_model in available_models
            else 0
        ),
        help="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    )

    if (
        previous_model != st.session_state.selected_model
        and st.session_state.session_initialized
    ):
        st.warning("âš ï¸ ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•˜ì„¸ìš”.")

    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
    st.session_state.timeout_seconds = st.slider(
        "â±ï¸ ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ)",
        min_value=60,
        max_value=300,
        value=st.session_state.timeout_seconds,
        step=10,
    )

    st.session_state.recursion_limit = st.slider(
        "â±ï¸ ì¬ê·€ í˜¸ì¶œ ì œí•œ(íšŸìˆ˜)",
        min_value=10,
        max_value=200,
        value=st.session_state.recursion_limit,
        step=10,
    )

    # ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •
    st.session_state.conversation_memory_turns = st.slider(
        "ğŸ§  ë©€í‹°í„´ ë©”ëª¨ë¦¬ ëŒ€í™” ìˆ˜",
        min_value=3,
        max_value=20,
        value=st.session_state.conversation_memory_turns,
        step=1,
        help="ì´ì „ ëŒ€í™”ë¥¼ ëª‡ í„´ê¹Œì§€ ê¸°ì–µí• ì§€ ì„¤ì •í•©ë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ ë” ë§ì€ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )

    st.divider()

    # ë„êµ¬ ì„¤ì • ì„¹ì…˜
    st.subheader("ğŸ”§ ì˜ë£Œ ë„êµ¬ ì„¤ì •")

    if "mcp_tools_expander" not in st.session_state:
        st.session_state.mcp_tools_expander = False

    with st.expander("ğŸ§° MCP ë„êµ¬ ì¶”ê°€", expanded=st.session_state.mcp_tools_expander):
        st.markdown("""
        [ì„¤ì • ê°€ì´ë“œ](https://teddylee777.notion.site/MCP-1d324f35d12980c8b018e12afdf545a1?pvs=4)
        
        âš ï¸ **ì¤‘ìš”**: JSONì„ ë°˜ë“œì‹œ ì¤‘ê´„í˜¸(`{}`)ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
        """)

        example_json = {
            "pubmed": {
                "command": "cmd",
                "args": [
                    "/c",
                    "npx",
                    "-y",
                    "@smithery/cli@latest",
                    "run",
                    "@JackKuo666/pubmed-mcp-server",
                    "--key",
                    "a0fde5b8-88e9-46d3-ab62-ad5096bd7d4b",
                ],
                "transport": "stdio",
            }
        }

        default_text = json.dumps(example_json, indent=2, ensure_ascii=False)

        new_tool_json = st.text_area(
            "ì˜ë£Œ ë„êµ¬ JSON",
            default_text,
            height=250,
        )

        if st.button("ë„êµ¬ ì¶”ê°€", type="primary", use_container_width=True):
            try:
                if not new_tool_json.strip().startswith("{") or not new_tool_json.strip().endswith("}"):
                    st.error("JSONì€ ì¤‘ê´„í˜¸({})ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ì•¼ í•©ë‹ˆë‹¤.")
                else:
                    parsed_tool = json.loads(new_tool_json)

                    if "mcpServers" in parsed_tool:
                        parsed_tool = parsed_tool["mcpServers"]

                    success_tools = []
                    for tool_name, tool_config in parsed_tool.items():
                        if "transport" not in tool_config:
                            tool_config["transport"] = "stdio"

                        if "command" not in tool_config:
                            st.error(f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'command' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                        elif "args" not in tool_config:
                            st.error(f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'args' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                        elif not isinstance(tool_config["args"], list):
                            st.error(f"'{tool_name}' ë„êµ¬ì˜ 'args' í•„ë“œëŠ” ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                        else:
                            st.session_state.pending_mcp_config[tool_name] = tool_config
                            success_tools.append(tool_name)

                    if success_tools:
                        if len(success_tools) == 1:
                            st.success(f"{success_tools[0]} ë„êµ¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        else:
                            tool_names = ", ".join(success_tools)
                            st.success(f"ì´ {len(success_tools)}ê°œ ë„êµ¬({tool_names})ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.session_state.mcp_tools_expander = False
                        st.rerun()
            except json.JSONDecodeError as e:
                st.error(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ë“±ë¡ëœ ë„êµ¬ ëª©ë¡
    with st.expander("ğŸ“‹ ë“±ë¡ëœ ì˜ë£Œ ë„êµ¬ ëª©ë¡", expanded=True):
        pending_config = st.session_state.pending_mcp_config
        
        if not pending_config:
            st.info("ì•„ì§ ë“±ë¡ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë„êµ¬ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.")
        else:
            for tool_name in list(pending_config.keys()):
                col1, col2 = st.columns([8, 2])
                
                # ê¸°ë³¸ ë„êµ¬ ì´ë¦„ í‘œì‹œ
                col1.markdown(f"- **{tool_name}**")
                
                # ì‹¤ì œ ì—°ê²°ëœ ë„êµ¬ì—ì„œ description ê°€ì ¸ì˜¤ê¸°
                if st.session_state.session_initialized:
                    tool_desc = get_tool_description_from_available_tools(tool_name)
                    if tool_desc:
                        col1.markdown(f"  <small style='color: #666;'>{tool_desc}</small>", unsafe_allow_html=True)
                else:
                    col1.markdown(f"  <small style='color: #999;'>ì—°ê²° í›„ ì„¤ëª…ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤</small>", unsafe_allow_html=True)
                
                # ì‚­ì œ ë²„íŠ¼
                if col2.button("ì‚­ì œ", key=f"delete_{tool_name}"):
                    del st.session_state.pending_mcp_config[tool_name]
                    st.success(f"{tool_name} ë„êµ¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()

    st.divider()

    # MCP ì„œë²„ ì—°ê²° ìƒíƒœ
    st.subheader("ğŸ”— MCP ì„œë²„ ì—°ê²° ìƒíƒœ")
    
    if st.session_state.server_status:
        for server_name, status_info in st.session_state.server_status.items():
            status = status_info["status"]
            tools_count = status_info["tools_count"]
            error_message = status_info["error_message"]
            
            # ìƒíƒœë³„ ì•„ì´ì½˜ê³¼ ìƒ‰ìƒ
            if status == "connected":
                status_icon = "âœ…"
                status_color = "success"
                status_text = f"ì—°ê²°ë¨ ({tools_count}ê°œ ë„êµ¬)"
            elif status == "connecting":
                status_icon = "ğŸ”„"
                status_color = "info"
                status_text = "ì—°ê²° ì¤‘..."
            else:  # failed
                status_icon = "âŒ"
                status_color = "error"
                status_text = "ì—°ê²° ì‹¤íŒ¨"
            
            with st.container():
                st.markdown(f"{status_icon} **{server_name}**: {status_text}")
                

                
                if error_message and status != "connected":
                    with st.expander(f"â— {server_name} ì˜¤ë¥˜ ì •ë³´", expanded=False):
                        st.error(error_message)
                        
                        # ì¼ë°˜ì ì¸ í•´ê²°ë°©ì•ˆ ì œì‹œ
                        st.markdown("**í•´ê²°ë°©ì•ˆ:**")
                        if "íƒ€ì„ì•„ì›ƒ" in error_message:
                            st.markdown("**ğŸ“¡ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ê°€ëŠ¥ì„±:**")
                            st.markdown("- ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
                            st.markdown("- VPN ì‚¬ìš© ì¤‘ì´ë©´ ì ì‹œ ë„ê³  ì‹œë„í•´ì£¼ì„¸ìš”")
                            st.markdown("- íšŒì‚¬/í•™êµ ë„¤íŠ¸ì›Œí¬ì—ì„œ npm íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œê°€ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                            st.markdown("- ë°©í™”ë²½ì´ë‚˜ í”„ë¡ì‹œ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
                            st.markdown("")
                            st.markdown("**ğŸ”§ Node.js í™˜ê²½ í™•ì¸:**")
                            st.markdown("- í„°ë¯¸ë„ì—ì„œ `node --version` ì‹¤í–‰")
                            st.markdown("- í„°ë¯¸ë„ì—ì„œ `npm --version` ì‹¤í–‰")
                            st.markdown("- Node.jsê°€ ì—†ë‹¤ë©´ https://nodejs.org ì—ì„œ ì„¤ì¹˜")
                        elif "npm" in error_message or "npx" in error_message:
                            st.markdown("- Node.jsê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”")
                            st.markdown("- `npm install -g @smithery/cli` ì‹¤í–‰í•´ë³´ì„¸ìš”")
                            st.markdown("- `npx -y @smithery/cli@latest run @JackKuo666/pubmed-mcp-server --key a0fde5b8-88e9-46d3-ab62-ad5096bd7d4b` ì§ì ‘ ì‹¤í–‰í•´ë³´ì„¸ìš”")
                        else:
                            st.markdown("- ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
                            st.markdown("- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
                            st.markdown("- PubMed API ì„œë²„ì˜ ì¼ì‹œì  ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    else:
        st.info("ì•„ì§ ì„œë²„ ì—°ê²°ì„ ì‹œë„í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ì—°ê²° ë¡œê·¸ í‘œì‹œ
    if st.session_state.connection_logs:
        with st.expander("ğŸ“‹ ì—°ê²° ë¡œê·¸", expanded=False):
            for log in st.session_state.connection_logs[-10:]:  # ìµœê·¼ 10ê°œë§Œ í‘œì‹œ
                st.text(log)

    st.divider()

    # ì‹œìŠ¤í…œ ì •ë³´
    st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    registered_tools = len(st.session_state.pending_mcp_config)
    connected_tools = st.session_state.get('tool_count', 0)
    
    st.write(f"ğŸ› ï¸ ë“±ë¡ëœ ë„êµ¬ ìˆ˜: {registered_tools}ê°œ")
    if st.session_state.session_initialized:
        st.write(f"ğŸ”— ì—°ê²°ëœ ë„êµ¬ ìˆ˜: {connected_tools}ê°œ")
        
        # ì „ì²´ ì—°ê²°ëœ ë„êµ¬ ëª©ë¡ ë³´ê¸° ë²„íŠ¼
        if connected_tools > 0:
            if st.button("ğŸ“‹ ì „ì²´ ë„êµ¬ ëª©ë¡", key="all_tools_button", help="ì—°ê²°ëœ ëª¨ë“  ë„êµ¬ì˜ ì„¤ëª… ë³´ê¸°"):
                st.session_state.show_all_tools = not st.session_state.get('show_all_tools', False)
            
            # ì „ì²´ ë„êµ¬ ëª©ë¡ í‘œì‹œ (ì„œë²„ë³„ êµ¬ë¶„)
            if st.session_state.get('show_all_tools', False):
                with st.expander("ğŸ”§ ì—°ê²°ëœ ëª¨ë“  ë„êµ¬ë“¤ (ì„œë²„ë³„)", expanded=True):
                    if st.session_state.get('available_tools'):
                        # ì„œë²„ë³„ë¡œ ë„êµ¬ë“¤ì„ ê·¸ë£¹í™”
                        server_tools_map = {}
                        
                        for tool in st.session_state.available_tools:
                            # ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ì„œë²„ ì¶”ì •
                            tool_server = "ê¸°íƒ€"
                            tool_name_lower = tool.name.lower()
                            
                            if "pubmed" in tool_name_lower or "medline" in tool_name_lower:
                                tool_server = "ğŸ“š PubMed"
                            elif any(keyword in tool_name_lower for keyword in ["search_", "webkr", "news", "blog", "shop", "image", "kin", "book", "encyc", "academic", "local", "cafe", "datalab"]):
                                tool_server = "ğŸ” ì›¹ê²€ìƒ‰"
                            elif "weather" in tool_name_lower or "climate" in tool_name_lower:
                                tool_server = "ğŸŒ¤ï¸ ë‚ ì”¨"
                            elif "time" in tool_name_lower or "clock" in tool_name_lower:
                                tool_server = "â° ì‹œê°„"
                            elif "calc" in tool_name_lower or "math" in tool_name_lower:
                                tool_server = "ğŸ§® ê³„ì‚°ê¸°"
                            else:
                                tool_server = "ğŸ”§ ê¸°íƒ€"
                            
                            if tool_server not in server_tools_map:
                                server_tools_map[tool_server] = []
                            server_tools_map[tool_server].append(tool)
                        
                        # ì„œë²„ë³„ë¡œ ë„êµ¬ í‘œì‹œ
                        for server, tools in server_tools_map.items():
                            st.markdown(f"## {server} ({len(tools)}ê°œ ë„êµ¬)")
                            
                            tools_with_desc = []
                            tools_without_desc = []
                            
                            for tool in tools:
                                if hasattr(tool, 'description') and tool.description and tool.description.strip():
                                    tools_with_desc.append(tool)
                                else:
                                    tools_without_desc.append(tool)
                            
                            # descriptionì´ ìˆëŠ” ë„êµ¬ë“¤
                            if tools_with_desc:
                                st.markdown("**ğŸ“ ì„¤ëª…ì´ ìˆëŠ” ë„êµ¬ë“¤:**")
                                for i, tool in enumerate(tools_with_desc, 1):
                                    with st.container():
                                        st.markdown(f"**{i}. {tool.name}**")
                                        st.markdown(f"ğŸ“ {tool.description}")
                                        
                                        # ë§¤ê°œë³€ìˆ˜ í‘œì‹œ
                                        if hasattr(tool, 'args_schema') and tool.args_schema:
                                            try:
                                                if hasattr(tool.args_schema, 'model_fields'):
                                                    fields = list(tool.args_schema.model_fields.keys())
                                                    if fields:
                                                        st.markdown(f"âš™ï¸ **ë§¤ê°œë³€ìˆ˜:** {', '.join(fields[:5])}{'...' if len(fields) > 5 else ''}")
                                            except:
                                                pass
                                        st.markdown("---")
                            
                            # descriptionì´ ì—†ëŠ” ë„êµ¬ë“¤
                            if tools_without_desc:
                                st.markdown(f"**ğŸ”§ ê¸°íƒ€ ë„êµ¬ë“¤:** {len(tools_without_desc)}ê°œ (ì„¤ëª… ì—†ìŒ)")
                                tool_names = [tool.name for tool in tools_without_desc]
                                st.markdown(f"ğŸ“‹ {', '.join(tool_names[:10])}{'...' if len(tool_names) > 10 else ''}")
                            
                            st.markdown("")  # ì„œë²„ ê°„ ê°„ê²©
                            st.markdown("---")  # ì„œë²„ ê°„ êµ¬ë¶„ì„ 
                        
                        if not server_tools_map:
                            st.info("ì—°ê²°ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.error("ë„êµ¬ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.write(f"ğŸ”— ì—°ê²°ëœ ë„êµ¬ ìˆ˜: ì´ˆê¸°í™” í•„ìš”")
    st.write(f"ğŸ§  í˜„ì¬ ëª¨ë¸: {st.session_state.selected_model}")
    
    # ëŒ€í™” ìƒíƒœ ì •ë³´
    conversation_turns = len([msg for msg in st.session_state.history if msg["role"] in ["user", "assistant"]]) // 2
    st.write(f"ğŸ’¬ í˜„ì¬ ëŒ€í™” í„´: {conversation_turns}í„´")
    st.write(f"ğŸ§  ë©”ëª¨ë¦¬ ì„¤ì •: ìµœëŒ€ {st.session_state.conversation_memory_turns}í„´ ê¸°ì–µ")

    # ì„¤ì • ì ìš©í•˜ê¸° ë²„íŠ¼
    if st.button("ì„¤ì • ì ìš©í•˜ê¸°", key="apply_button", type="primary", use_container_width=True):
        apply_status = st.empty()
        with apply_status.container():
            st.warning("ğŸ”„ ì˜ë£Œ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            progress_bar = st.progress(0)
            status_text = st.empty()

            # 0ë‹¨ê³„: ì—°ê²° ìƒíƒœ ì´ˆê¸°í™” (ì‚¬ì´ë“œë°” ê°±ì‹ ì„ ìœ„í•´)
            status_text.text("ì—°ê²° ìƒíƒœ ì´ˆê¸°í™” ì¤‘...")
            st.session_state.server_status = {}  # ì„œë²„ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
            st.session_state.connection_logs = []  # ì—°ê²° ë¡œê·¸ ì´ˆê¸°í™”
            progress_bar.progress(10)

            # 1ë‹¨ê³„: ì„¤ì • ì €ì¥
            status_text.text("ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘...")
            save_result = save_config_to_json(st.session_state.pending_mcp_config)
            progress_bar.progress(25)

            # 2ë‹¨ê³„: ê¸°ì¡´ ì—°ê²° ì •ë¦¬
            status_text.text("ê¸°ì¡´ ì—°ê²° ì •ë¦¬ ì¤‘...")
            st.session_state.session_initialized = False
            st.session_state.agent = None
            st.session_state.available_tools = []  # ê¸°ì¡´ ë„êµ¬ ëª©ë¡ ì •ë¦¬
            st.session_state.tool_count = 0
            progress_bar.progress(40)

            # 3ë‹¨ê³„: ê° ì„œë²„ë¥¼ ì—°ê²° ëŒ€ê¸° ìƒíƒœë¡œ ì„¤ì •
            status_text.text("ì„œë²„ ì—°ê²° ì¤€ë¹„ ì¤‘...")
            for server_name in st.session_state.pending_mcp_config.keys():
                update_server_status(server_name, "connecting", "ì—°ê²° ì¤€ë¹„ ì¤‘...")
            progress_bar.progress(50)

            # 4ë‹¨ê³„: MCP ì„œë²„ ì—°ê²° ì‹œë„
            status_text.text("MCP ì„œë²„ ì—°ê²° ì¤‘...")
            progress_bar.progress(60)
            
            success = st.session_state.event_loop.run_until_complete(
                initialize_session(st.session_state.pending_mcp_config)
            )
            progress_bar.progress(100)

            if success:
                status_text.text("ì´ˆê¸°í™” ì™„ë£Œ!")
                st.success("âœ… ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                if "mcp_tools_expander" in st.session_state:
                    st.session_state.mcp_tools_expander = False
            else:
                status_text.text("ì´ˆê¸°í™” ì‹¤íŒ¨")
                st.error("âŒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        st.rerun()
    
    # ì¬ì—°ê²° ë²„íŠ¼ (ì—°ê²° ì‹¤íŒ¨í•œ ì„œë²„ê°€ ìˆì„ ë•Œë§Œ í‘œì‹œ)
    failed_servers = [name for name, status in st.session_state.server_status.items() 
                     if status["status"] == "failed"]
    
    if failed_servers:
        if st.button("ğŸ”„ ì‹¤íŒ¨í•œ ì„œë²„ ì¬ì—°ê²°", key="reconnect_button", use_container_width=True):
            with st.spinner("ğŸ”„ ì‹¤íŒ¨í•œ ì„œë²„ë“¤ ì¬ì—°ê²° ì¤‘..."):
                # ì‹¤íŒ¨í•œ ì„œë²„ë“¤ì˜ ìƒíƒœë¥¼ ì—°ê²° ì‹œë„ë¡œ ë³€ê²½
                for server_name in failed_servers:
                    update_server_status(server_name, "connecting", "ì¬ì—°ê²° ì‹œë„ ì¤‘...")
                
                # ì—°ê²° ë¡œê·¸ ì¶”ê°€
                st.session_state.connection_logs.append(f"[ì¬ì—°ê²°] {len(failed_servers)}ê°œ ì„œë²„ ì¬ì—°ê²° ì‹œë„")
                
                # ì¬ì—°ê²° ì‹œë„
                success = st.session_state.event_loop.run_until_complete(
                    initialize_session(st.session_state.pending_mcp_config)
                )
                
                if success:
                    st.success("âœ… ì¬ì—°ê²°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.warning("âš ï¸ ì¼ë¶€ ì„œë²„ ì¬ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            st.rerun()

    st.divider()

    # ì‘ì—… ë²„íŠ¼ë“¤
    st.subheader("ğŸ”„ ì‘ì—…")

    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True, type="primary"):
        # ëŒ€í™” ê¸°ë¡ ë° ë©”ëª¨ë¦¬ ì™„ì „ ì´ˆê¸°í™”
        st.session_state.thread_id = random_uuid()
        st.session_state.history = []
        
        # LangGraph ë©”ëª¨ë¦¬ë„ ìƒˆë¡œìš´ thread_idë¡œ ì´ˆê¸°í™”ë¨
        st.success("âœ… ëŒ€í™” ê¸°ë¡ê³¼ ë©”ëª¨ë¦¬ê°€ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()
        
    # ëŒ€í™” ìš”ì•½ ë²„íŠ¼ (ëŒ€í™”ê°€ ë§ì„ ë•Œë§Œ í‘œì‹œ)
    conversation_turns = len([msg for msg in st.session_state.history if msg["role"] in ["user", "assistant"]]) // 2
    if conversation_turns > st.session_state.conversation_memory_turns:
        if st.button("ğŸ“ ì˜¤ë˜ëœ ëŒ€í™” ìš”ì•½", use_container_width=True):
            st.info("ğŸ’¡ í˜„ì¬ ì„¤ì •ëœ ë©”ëª¨ë¦¬ í„´ ìˆ˜ë¥¼ ì´ˆê³¼í•œ ëŒ€í™”ëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.")
            st.info(f"ğŸ”„ í˜„ì¬ {conversation_turns}í„´ ì¤‘ ìµœê·¼ {st.session_state.conversation_memory_turns}í„´ë§Œ ê¸°ì–µí•©ë‹ˆë‹¤.")

# --- ë©”ì¸ ì»¨í…ì¸  ---
# ëŒ€í™” ìƒíƒœ í‘œì‹œ
conversation_turns = len([msg for msg in st.session_state.history if msg["role"] in ["user", "assistant"]]) // 2

if not st.session_state.session_initialized:
    if st.session_state.basic_model_initialized:
        if conversation_turns > 0:
            st.info(f"ğŸ’¬ ê¸°ë³¸ ëª¨ë¸ ëŒ€í™” ì§„í–‰ ì¤‘ ({conversation_turns}í„´) - ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤!")
        else:
            st.info("ğŸ’¬ ê¸°ë³¸ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ì§€ê¸ˆ ë°”ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.warning("ğŸ”§ ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì˜ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
    else:
        st.info("ğŸ¥ ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì˜ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
else:
    workflow_type = st.session_state.get('default_workflow_type', 'general')
    if conversation_turns > 0:
        st.success(f"âœ… AI ì—ì´ì „íŠ¸ ì¤€ë¹„ ì™„ë£Œ! í˜„ì¬ {conversation_turns}í„´ ëŒ€í™” ì§„í–‰ ì¤‘ (ê¸°ë³¸: {workflow_type} ì›Œí¬í”Œë¡œìš°)")
        st.info("ğŸ§  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ë©° ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.")
    else:
        st.success(f"âœ… AI ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! (ê¸°ë³¸: {workflow_type} ì›Œí¬í”Œë¡œìš°) í•œê¸€ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_message()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# ëŒ€í™” ìƒíƒœì— ë”°ë¥¸ í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ì„¤ì •
if conversation_turns > 0:
    if st.session_state.session_initialized:
        placeholder_text = f"ğŸ’¬ ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (í˜„ì¬ {conversation_turns}í„´, ì´ì „ ëŒ€í™” ê¸°ì–µí•¨)"
    else:
        placeholder_text = f"ğŸ’¬ ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (í˜„ì¬ {conversation_turns}í„´, ê¸°ë³¸ ëª¨ë¸)"
else:
    placeholder_text = "ğŸ’¬ ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (í•œê¸€ ê°€ëŠ¥)"

user_query = st.chat_input(placeholder_text)
if user_query:
    st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(user_query)
    
    if st.session_state.session_initialized:
        # ì™„ì „í•œ ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‚¬ìš©
        with st.chat_message("assistant", avatar="ğŸ¥"):
            tool_placeholder = st.empty()
            text_placeholder = st.empty()
            
            resp, final_text, final_tool = (
                st.session_state.event_loop.run_until_complete(
                    process_query(
                        user_query,
                        text_placeholder,
                        tool_placeholder,
                        st.session_state.timeout_seconds,
                    )
                )
            )
        
        if "error" in resp:
            st.error(resp["error"])
        else:
            st.session_state.history.append({"role": "user", "content": user_query})
            st.session_state.history.append({"role": "assistant", "content": final_text})
            if final_tool.strip():
                st.session_state.history.append({"role": "assistant_tool", "content": final_tool})
            st.rerun()
    
    elif st.session_state.basic_model_initialized:
        # ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            text_placeholder = st.empty()
            
            resp, final_text, final_tool = (
                st.session_state.event_loop.run_until_complete(
                    process_basic_query(
                        user_query,
                        text_placeholder,
                        st.session_state.timeout_seconds,
                    )
                )
            )
        
        if "error" in resp:
            st.error(resp["error"])
        else:
            st.session_state.history.append({"role": "user", "content": user_query})
            st.session_state.history.append({"role": "assistant", "content": final_text})
            
            # ë©€í‹°í„´ ëŒ€í™” ì •ë³´ í‘œì‹œ
            new_turn_count = len([msg for msg in st.session_state.history if msg["role"] in ["user", "assistant"]]) // 2
            if new_turn_count > 1:
                st.info(f"â„¹ï¸ ê¸°ë³¸ ëª¨ë¸ë¡œ ë‹µë³€ ({new_turn_count}í„´ì§¸, ì´ì „ ëŒ€í™” ê¸°ì–µí•¨). ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ 'ì„¤ì • ì ìš©í•˜ê¸°'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            else:
                st.info("â„¹ï¸ ê¸°ë³¸ ëª¨ë¸ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì˜ë£Œ ë…¼ë¬¸ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ 'ì„¤ì • ì ìš©í•˜ê¸°'ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            st.rerun()
    
    else:
        st.warning("âš ï¸ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
